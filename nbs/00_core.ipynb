{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The foundations\n",
    "\n",
    "> Let's try to replicate the standard usage of MNL frameworks\n",
    "\n",
    "We will try to implement a basic MNL package to compare against biogeme/others... let's start from this [blog post](https://aaronkub.com/2020/02/12/logistic-regression-with-pytorch.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "from fastprogress import progress_bar, master_bar\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grab some data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"./data/Iris.csv\").drop(\"Id\", axis=1)\n",
    "\n",
    "X_numpy = data.drop(\"Species\", axis=1).values\n",
    "\n",
    "target_map = {\n",
    "    val: index for index, val in enumerate(data.Species.unique())\n",
    "}\n",
    "y_numpy = data.Species.map(target_map).values\n",
    "\n",
    "X = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "y = torch.tensor(y_numpy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SepalLengthCm</th>\n",
       "      <th>SepalWidthCm</th>\n",
       "      <th>PetalLengthCm</th>\n",
       "      <th>PetalWidthCm</th>\n",
       "      <th>Species</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   SepalLengthCm  SepalWidthCm  PetalLengthCm  PetalWidthCm      Species\n",
       "0            5.1           3.5            1.4           0.2  Iris-setosa\n",
       "1            4.9           3.0            1.4           0.2  Iris-setosa\n",
       "2            4.7           3.2            1.3           0.2  Iris-setosa\n",
       "3            4.6           3.1            1.5           0.2  Iris-setosa\n",
       "4            5.0           3.6            1.4           0.2  Iris-setosa"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def prepare_data(data, x_cols=None, target_col=None, av_cols=None, target_map=None):\n",
    "    \"This is far from optimal, as we shuould be reading values lazily\"\n",
    "    target_col = ifnone(target_col, list(data.columns)[-1])\n",
    "    x_cols = [col for col in ifnone(x_cols, list(data.columns)) if col!=target_col]\n",
    "    X_numpy = data.loc[:, x_cols].values\n",
    "    default_target_map = {\n",
    "        val: index for index, val in enumerate(data.loc[:,target_col].unique())\n",
    "    }\n",
    "    target_map = ifnone(target_map, default_target_map)\n",
    "    print(target_map)\n",
    "    y_numpy = data.loc[:,target_col].map(target_map).values\n",
    "    \n",
    "    X = torch.tensor(X_numpy, dtype=torch.float32)\n",
    "    y = torch.tensor(y_numpy)\n",
    "    if av_cols is None:\n",
    "        return X, y, torch.ones(len(X), len(target_map))\n",
    "    else:\n",
    "        return X, y, torch.tensor(data.loc[:, av_cols].values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Iris-setosa': 0, 'Iris-versicolor': 1, 'Iris-virginica': 2}\n"
     ]
    }
   ],
   "source": [
    "X, y, available_choices = prepare_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = TensorDataset(X, y, available_choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5.1000, 3.5000, 1.4000, 0.2000],\n",
       "         [4.9000, 3.0000, 1.4000, 0.2000],\n",
       "         [4.7000, 3.2000, 1.3000, 0.2000],\n",
       "         [4.6000, 3.1000, 1.5000, 0.2000],\n",
       "         [5.0000, 3.6000, 1.4000, 0.2000]]),\n",
       " tensor([0, 0, 0, 0, 0]),\n",
       " tensor([[1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.],\n",
       "         [1., 1., 1.]]))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0:5], y[0:5], available_choices[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 150)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(range_of(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def train_valid_split(*tensors, pct=0.2, shuffle=True):\n",
    "    assert len(tensors[0]) == len(tensors[1]), \"X and y don't have the same number of elements\"\n",
    "    indices = range_of(tensors[0])\n",
    "    if shuffle:\n",
    "        random.shuffle(indices)\n",
    "    n = len(tensors[0])\n",
    "    n_train = int(n * (1-0.2))\n",
    "    train_tensors = tuple(t[indices[:n_train]] for t in tensors)\n",
    "    valid_tensors = tuple(t[indices[n_train:]] for t in tensors)\n",
    "    return train_tensors, valid_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_X = L([0,1,2,3,4,5,6,7,8,9])\n",
    "_y = L('a,b,c,d,e,f,g,h,i,j'.split(','))\n",
    "\n",
    "test_eq(train_valid_split(_X,_y, shuffle=False)[0][0], [0,1,2,3,4,5,6,7])\n",
    "test_eq(train_valid_split(_X,_y, shuffle=False)[1][1], ['i','j'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Availability(nn.Module):\n",
    "    \n",
    "    def __init__(self, n_choices, inf=1e6):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        \n",
    "    def forward(self, x, av):\n",
    "        return x + (av - 1)*self.inf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "av = Availability(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2689, 0.7311, 0.0000])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(av(torch.tensor([1,2,3]), torch.tensor([1,1,0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class LinearMNL(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dim=4, out_dim=5, bias=False):\n",
    "        super().__init__()\n",
    "        store_attr()\n",
    "        self.linear = nn.Linear(in_dim, out_dim, bias=bias)\n",
    "        self.availability = Availability(out_dim)\n",
    "        \n",
    "    def forward(self, x, av):\n",
    "        return self.availability(self.linear(x), av)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 3])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearMNL(4,3)\n",
    "x = torch.rand(10,4)\n",
    "\n",
    "model(x, torch.tensor([1,1,1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class DataLoaders:\n",
    "    \"\"\"\n",
    "    A class to store dataloaders (train/valid/test....)\"\"\"\n",
    "    def __init__(self, train_dl, valid_dl=None):\n",
    "        self.train_dl = train_dl\n",
    "        self.valid_dl = ifnone(valid_dl, train_dl)\n",
    "        \n",
    "    def one_batch(self, dl=None):\n",
    "        dl = ifnone(dl, self.train_dl)\n",
    "        return next(iter(dl))\n",
    "    \n",
    "    @delegates(DataLoader, but='batch_size')\n",
    "    @classmethod\n",
    "    def from_datasets(cls, train_ds, valid_ds=None, batch_size=1, **kwargs):\n",
    "        train_dl = DataLoader(train_ds, batch_size=batch_size, **kwargs)\n",
    "        if valid_ds is not None:\n",
    "            valid_dl = DataLoader(valid_ds, batch_size=2*batch_size, **kwargs)\n",
    "        else:\n",
    "            valid_dl = None\n",
    "        return cls(train_dl, valid_dl)\n",
    "    \n",
    "    @delegates(DataLoader, but='batch_size')\n",
    "    @classmethod\n",
    "    def from_Xy(cls, X, y, available_choices, pct=None, batch_size=1, **kwargs):\n",
    "        if pct is not None:\n",
    "            train_tensors, valid_tensors = train_valid_split(X, y, available_choices, pct=pct)\n",
    "        else:\n",
    "            train_tensors = X, y, available_choices\n",
    "            valid_tensors = None\n",
    "        train_ds = TensorDataset(*train_tensors)\n",
    "        if valid_tensors is not None:\n",
    "            valid_ds = TensorDataset(*train_tensors)\n",
    "        else:\n",
    "            valid_ds = None\n",
    "        return cls.from_datasets(train_ds, valid_ds, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class EarlyStopping():\n",
    "    \"\"\"\n",
    "    Early stopping to stop the training when the loss does not improve after\n",
    "    certain epochs.\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=10, min_delta=1e-4):\n",
    "        \"\"\"\n",
    "        :param patience: how many epochs to wait before stopping when loss is\n",
    "               not improving\n",
    "        :param min_delta: minimum difference between new loss and old loss for\n",
    "               new loss to be considered as an improvement\n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss == None:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "        elif self.best_loss - val_loss < self.min_delta:\n",
    "            self.counter += 1\n",
    "            print(f\"INFO: Early stopping counter {self.counter} of {self.patience}\")\n",
    "            if self.counter >= self.patience:\n",
    "                print('INFO: Early stopping')\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class Learner:\n",
    "    \"A wrapper around dls, model and optimizer\"\n",
    "    def __init__(self, dls, model, loss_func=torch.nn.CrossEntropyLoss(reduction='sum')):\n",
    "        store_attr()\n",
    "        \n",
    "\n",
    "    def train_one_epoch(self):\n",
    "        train_loss = 0.\n",
    "        for batch, (x, y, av) in enumerate(self.dls.train_dl):\n",
    "            pred = self.model(x, av)  # 1\n",
    "            loss = self.loss_func(pred, y)\n",
    "\n",
    "            #backprop\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "        return train_loss\n",
    "    \n",
    "    def validate(self, dl=None):\n",
    "        dl = ifnone(dl, self.dls.valid_dl)\n",
    "        val_loss, accu = 0, 0\n",
    "        preds = []\n",
    "        with torch.no_grad():\n",
    "            for batch, (x, y, av) in enumerate(dl):\n",
    "                pred = self.model(x, av)\n",
    "                val_loss += self.loss_func(pred, y).item()\n",
    "                accu += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "                preds += [pred]\n",
    "        return torch.cat(preds), val_loss, accu / len(dl.dataset)\n",
    "    \n",
    "    def fit(self, n_epochs=10, lr=0.01, wd=0.01, patience=10):\n",
    "        \n",
    "        self.optimizer = torch.optim.SGD(\n",
    "            self.model.parameters(), \n",
    "            lr=lr, \n",
    "            weight_decay=wd\n",
    "        )\n",
    "        \n",
    "        initial_loss = _, val_loss, accuracy = self.validate()\n",
    "        print(f'Starting fit model: \\nInitial val_loss = {val_loss:.3f}, accuracy = {accuracy:.2f}')\n",
    "        \n",
    "        early_stopping = EarlyStopping(patience=patience)\n",
    "        \n",
    "        scheduler = ReduceLROnPlateau(self.optimizer, 'min', patience=patience, threshold=1e-4, verbose=True)\n",
    "        \n",
    "        for epoch in progress_bar(range_of(n_epochs), leave=False):\n",
    "            train_loss = self.train_one_epoch()\n",
    "            _, val_loss, accuracy = self.validate()\n",
    "            scheduler.step(val_loss)\n",
    "            early_stopping(val_loss)\n",
    "            if early_stopping.early_stop:\n",
    "                break\n",
    "            print(f'epoch = {epoch:3.0f}, train_loss = {train_loss:.3f}, val_loss = {val_loss:.3f}, accuracy = {accuracy:.2f}')\n",
    "            \n",
    "    def get_preds(self, dl=None, with_loss=False):\n",
    "        dl = ifnone(dl, self.dls.valid_dl)\n",
    "        preds, val_loss, accu = self.validate(dl)\n",
    "        _, targets, _ = dl.dataset.tensors\n",
    "        return (preds, targets, val_loss) if with_loss else (preds, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dls = DataLoaders.from_Xy(X, y, available_choices, pct=0.2, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearMNL(4,3)\n",
    "learn = Learner(dls, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fit model: \n",
      "Initial val_loss = 408.729, accuracy = 0.36\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch =   0, train_loss = 171.844, val_loss = 114.944, accuracy = 0.64\n",
      "epoch =   1, train_loss = 123.184, val_loss = 78.223, accuracy = 0.67\n",
      "epoch =   2, train_loss = 108.055, val_loss = 62.750, accuracy = 0.68\n",
      "epoch =   3, train_loss = 98.625, val_loss = 55.262, accuracy = 0.78\n",
      "epoch =   4, train_loss = 91.540, val_loss = 50.811, accuracy = 0.84\n",
      "epoch =   5, train_loss = 85.711, val_loss = 47.720, accuracy = 0.87\n",
      "epoch =   6, train_loss = 80.656, val_loss = 45.389, accuracy = 0.88\n",
      "epoch =   7, train_loss = 76.148, val_loss = 43.605, accuracy = 0.88\n",
      "epoch =   8, train_loss = 72.076, val_loss = 42.304, accuracy = 0.88\n",
      "epoch =   9, train_loss = 68.380, val_loss = 41.455, accuracy = 0.88\n",
      "epoch =  10, train_loss = 65.018, val_loss = 41.007, accuracy = 0.86\n",
      "epoch =  11, train_loss = 61.951, val_loss = 40.863, accuracy = 0.85\n",
      "INFO: Early stopping counter 1 of 10\n",
      "epoch =  12, train_loss = 59.138, val_loss = 40.893, accuracy = 0.84\n",
      "INFO: Early stopping counter 2 of 10\n",
      "epoch =  13, train_loss = 56.541, val_loss = 40.959, accuracy = 0.84\n",
      "INFO: Early stopping counter 3 of 10\n",
      "epoch =  14, train_loss = 54.131, val_loss = 40.950, accuracy = 0.84\n",
      "epoch =  15, train_loss = 51.885, val_loss = 40.790, accuracy = 0.83\n",
      "epoch =  16, train_loss = 49.786, val_loss = 40.448, accuracy = 0.84\n",
      "epoch =  17, train_loss = 47.825, val_loss = 39.925, accuracy = 0.84\n",
      "epoch =  18, train_loss = 45.996, val_loss = 39.245, accuracy = 0.84\n",
      "epoch =  19, train_loss = 44.292, val_loss = 38.445, accuracy = 0.84\n",
      "epoch =  20, train_loss = 42.709, val_loss = 37.563, accuracy = 0.85\n",
      "epoch =  21, train_loss = 41.243, val_loss = 36.636, accuracy = 0.86\n",
      "epoch =  22, train_loss = 39.886, val_loss = 35.693, accuracy = 0.87\n",
      "epoch =  23, train_loss = 38.634, val_loss = 34.758, accuracy = 0.87\n",
      "epoch =  24, train_loss = 37.479, val_loss = 33.848, accuracy = 0.88\n",
      "epoch =  25, train_loss = 36.414, val_loss = 32.974, accuracy = 0.88\n",
      "epoch =  26, train_loss = 35.431, val_loss = 32.142, accuracy = 0.88\n",
      "epoch =  27, train_loss = 34.523, val_loss = 31.356, accuracy = 0.89\n",
      "epoch =  28, train_loss = 33.683, val_loss = 30.618, accuracy = 0.89\n",
      "epoch =  29, train_loss = 32.905, val_loss = 29.925, accuracy = 0.91\n",
      "epoch =  30, train_loss = 32.183, val_loss = 29.278, accuracy = 0.92\n",
      "epoch =  31, train_loss = 31.512, val_loss = 28.672, accuracy = 0.92\n",
      "epoch =  32, train_loss = 30.887, val_loss = 28.106, accuracy = 0.92\n",
      "epoch =  33, train_loss = 30.303, val_loss = 27.577, accuracy = 0.92\n",
      "epoch =  34, train_loss = 29.758, val_loss = 27.081, accuracy = 0.92\n",
      "epoch =  35, train_loss = 29.248, val_loss = 26.618, accuracy = 0.92\n",
      "epoch =  36, train_loss = 28.769, val_loss = 26.183, accuracy = 0.92\n",
      "epoch =  37, train_loss = 28.319, val_loss = 25.775, accuracy = 0.92\n",
      "epoch =  38, train_loss = 27.896, val_loss = 25.391, accuracy = 0.93\n",
      "epoch =  39, train_loss = 27.498, val_loss = 25.030, accuracy = 0.93\n",
      "epoch =  40, train_loss = 27.122, val_loss = 24.690, accuracy = 0.93\n",
      "epoch =  41, train_loss = 26.767, val_loss = 24.368, accuracy = 0.93\n",
      "epoch =  42, train_loss = 26.431, val_loss = 24.065, accuracy = 0.93\n",
      "epoch =  43, train_loss = 26.113, val_loss = 23.777, accuracy = 0.93\n",
      "epoch =  44, train_loss = 25.811, val_loss = 23.505, accuracy = 0.94\n",
      "epoch =  45, train_loss = 25.525, val_loss = 23.246, accuracy = 0.94\n",
      "epoch =  46, train_loss = 25.253, val_loss = 23.001, accuracy = 0.94\n",
      "epoch =  47, train_loss = 24.995, val_loss = 22.767, accuracy = 0.94\n",
      "epoch =  48, train_loss = 24.749, val_loss = 22.544, accuracy = 0.94\n",
      "epoch =  49, train_loss = 24.514, val_loss = 22.332, accuracy = 0.94\n",
      "epoch =  50, train_loss = 24.290, val_loss = 22.129, accuracy = 0.94\n",
      "epoch =  51, train_loss = 24.077, val_loss = 21.935, accuracy = 0.95\n",
      "epoch =  52, train_loss = 23.872, val_loss = 21.749, accuracy = 0.95\n",
      "epoch =  53, train_loss = 23.677, val_loss = 21.572, accuracy = 0.95\n",
      "epoch =  54, train_loss = 23.490, val_loss = 21.401, accuracy = 0.96\n",
      "epoch =  55, train_loss = 23.311, val_loss = 21.238, accuracy = 0.96\n",
      "epoch =  56, train_loss = 23.139, val_loss = 21.081, accuracy = 0.96\n",
      "epoch =  57, train_loss = 22.973, val_loss = 20.930, accuracy = 0.96\n",
      "epoch =  58, train_loss = 22.815, val_loss = 20.785, accuracy = 0.96\n",
      "epoch =  59, train_loss = 22.662, val_loss = 20.645, accuracy = 0.96\n",
      "epoch =  60, train_loss = 22.515, val_loss = 20.510, accuracy = 0.96\n",
      "epoch =  61, train_loss = 22.374, val_loss = 20.380, accuracy = 0.96\n",
      "epoch =  62, train_loss = 22.238, val_loss = 20.255, accuracy = 0.96\n",
      "epoch =  63, train_loss = 22.106, val_loss = 20.134, accuracy = 0.96\n",
      "epoch =  64, train_loss = 21.980, val_loss = 20.017, accuracy = 0.96\n",
      "epoch =  65, train_loss = 21.857, val_loss = 19.904, accuracy = 0.96\n",
      "epoch =  66, train_loss = 21.739, val_loss = 19.795, accuracy = 0.96\n",
      "epoch =  67, train_loss = 21.625, val_loss = 19.689, accuracy = 0.96\n",
      "epoch =  68, train_loss = 21.514, val_loss = 19.587, accuracy = 0.96\n",
      "epoch =  69, train_loss = 21.407, val_loss = 19.487, accuracy = 0.96\n",
      "epoch =  70, train_loss = 21.303, val_loss = 19.391, accuracy = 0.96\n",
      "epoch =  71, train_loss = 21.203, val_loss = 19.298, accuracy = 0.96\n",
      "epoch =  72, train_loss = 21.105, val_loss = 19.207, accuracy = 0.96\n",
      "epoch =  73, train_loss = 21.011, val_loss = 19.119, accuracy = 0.96\n",
      "epoch =  74, train_loss = 20.919, val_loss = 19.034, accuracy = 0.96\n",
      "epoch =  75, train_loss = 20.830, val_loss = 18.951, accuracy = 0.96\n",
      "epoch =  76, train_loss = 20.743, val_loss = 18.871, accuracy = 0.96\n",
      "epoch =  77, train_loss = 20.659, val_loss = 18.792, accuracy = 0.96\n",
      "epoch =  78, train_loss = 20.577, val_loss = 18.716, accuracy = 0.96\n",
      "epoch =  79, train_loss = 20.498, val_loss = 18.642, accuracy = 0.96\n",
      "epoch =  80, train_loss = 20.420, val_loss = 18.570, accuracy = 0.96\n",
      "epoch =  81, train_loss = 20.345, val_loss = 18.500, accuracy = 0.96\n",
      "epoch =  82, train_loss = 20.271, val_loss = 18.431, accuracy = 0.96\n",
      "epoch =  83, train_loss = 20.199, val_loss = 18.365, accuracy = 0.96\n",
      "epoch =  84, train_loss = 20.130, val_loss = 18.300, accuracy = 0.96\n",
      "epoch =  85, train_loss = 20.062, val_loss = 18.236, accuracy = 0.96\n",
      "epoch =  86, train_loss = 19.995, val_loss = 18.175, accuracy = 0.96\n",
      "epoch =  87, train_loss = 19.930, val_loss = 18.114, accuracy = 0.96\n",
      "epoch =  88, train_loss = 19.867, val_loss = 18.055, accuracy = 0.96\n",
      "epoch =  89, train_loss = 19.805, val_loss = 17.998, accuracy = 0.96\n",
      "epoch =  90, train_loss = 19.745, val_loss = 17.942, accuracy = 0.96\n",
      "epoch =  91, train_loss = 19.686, val_loss = 17.887, accuracy = 0.96\n",
      "epoch =  92, train_loss = 19.628, val_loss = 17.834, accuracy = 0.96\n",
      "epoch =  93, train_loss = 19.572, val_loss = 17.781, accuracy = 0.96\n",
      "epoch =  94, train_loss = 19.517, val_loss = 17.730, accuracy = 0.96\n",
      "epoch =  95, train_loss = 19.463, val_loss = 17.680, accuracy = 0.96\n",
      "epoch =  96, train_loss = 19.410, val_loss = 17.632, accuracy = 0.96\n",
      "epoch =  97, train_loss = 19.359, val_loss = 17.584, accuracy = 0.96\n",
      "epoch =  98, train_loss = 19.308, val_loss = 17.537, accuracy = 0.96\n",
      "epoch =  99, train_loss = 19.259, val_loss = 17.491, accuracy = 0.96\n"
     ]
    }
   ],
   "source": [
    "learn.fit(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_core.ipynb.\n",
      "Converted index.ipynb.\n",
      "Converted swissmetro.ipynb.\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.export import *\n",
    "notebook2script()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
